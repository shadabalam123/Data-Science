{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Web Scraping Project using Python.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNIWxbE9ZTJenHuop8KOIuI"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"At_3At6vjjjs","executionInfo":{"status":"ok","timestamp":1624964272797,"user_tz":-330,"elapsed":553,"user":{"displayName":"Shadab Alam","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgyC0IjSD4d7VtA_QxL79JaEuLCgySEzKHr-_4KUQ=s64","userId":"12256271489578079776"}}},"source":["# import the library to query a website\n","import requests\n","# specify the URL\n","wiki_link= \"https://en.wikipedia.org/wiki/List_of_Asian_countries_by_area\"\n","link=requests.get(wiki_link)"],"execution_count":30,"outputs":[]},{"cell_type":"code","metadata":{"id":"2WqIQJYRkdC2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1624964273504,"user_tz":-330,"elapsed":39,"user":{"displayName":"Shadab Alam","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgyC0IjSD4d7VtA_QxL79JaEuLCgySEzKHr-_4KUQ=s64","userId":"12256271489578079776"}},"outputId":"96911c21-80f3-4aff-9a64-2f3adcaf3039"},"source":["print(link) \n","\n","# By printing link , you can see that the HTTP link status code is 200, which means that the request for the URL was successful:"],"execution_count":31,"outputs":[{"output_type":"stream","text":["<Response [200]>\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"9bheK8VDOONz"},"source":["# But we need the HTML content of the requested web page, so as the next step we retrieve the content of the webpage using text or content  parameter:\n","\n","link1=requests.get(wiki_link).text\n","print(link1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1d3y7LXfkjWB","executionInfo":{"status":"ok","timestamp":1624964274885,"user_tz":-330,"elapsed":250,"user":{"displayName":"Shadab Alam","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgyC0IjSD4d7VtA_QxL79JaEuLCgySEzKHr-_4KUQ=s64","userId":"12256271489578079776"}}},"source":["# next task is to parse the data return from the website . This can be done using Beautiful function .\n","# Parsing is done to understand the building blocks of HTML document , such as <head>,<body>,<title>,<h1>,e.t.c.\n","\n","from bs4 import BeautifulSoup\n","soup=BeautifulSoup(link1,\"lxml\")\n","\n","# The first parameter of the BeautifulSoup() method is link1 \n","# (which was the variable where we saved that hard-to-read HTML content from the fetched wiki_link URL),\n","# the second parameter (“lxml”) is the parser that is used on the link1 variable.\n","# apart from the lxml parser python has the built in parser that is \"html.parser\". However lxml is preferred for its better speed than html.parser"],"execution_count":33,"outputs":[]},{"cell_type":"code","metadata":{"id":"BvJI3e_nRVoh"},"source":["print(soup)\n","# it looks more pleasing to our eyes than earlier,but still we are missing nested structure. This will help us to see the structure .\n","# html tags . so we will be using prettify function to know about the diffrent available tags so as to extract the information from it"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sQv7pvHtRfFp"},"source":["print(soup.prettify())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5H-93TVjUSne","executionInfo":{"status":"ok","timestamp":1624964274891,"user_tz":-330,"elapsed":126,"user":{"displayName":"Shadab Alam","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgyC0IjSD4d7VtA_QxL79JaEuLCgySEzKHr-_4KUQ=s64","userId":"12256271489578079776"}}},"source":["# Next we wil be fetching information from the different HTML tags \n","# HTML consists of elements like links, paragraphs, headings, blocks, etc.\n","# These elements are wrapped between tags; inside the opening and the closing tag can be found the content of the element."],"execution_count":36,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Bs2PUMm-Uwnk","executionInfo":{"status":"ok","timestamp":1624964274893,"user_tz":-330,"elapsed":128,"user":{"displayName":"Shadab Alam","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgyC0IjSD4d7VtA_QxL79JaEuLCgySEzKHr-_4KUQ=s64","userId":"12256271489578079776"}},"outputId":"67afc285-9854-466d-ac66-60fbd5805064"},"source":["soup.title"],"execution_count":37,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<title>List of Asian countries by area - Wikipedia</title>"]},"metadata":{"tags":[]},"execution_count":37}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"7b6-gdTGU3aB","executionInfo":{"status":"ok","timestamp":1624964274894,"user_tz":-330,"elapsed":22,"user":{"displayName":"Shadab Alam","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgyC0IjSD4d7VtA_QxL79JaEuLCgySEzKHr-_4KUQ=s64","userId":"12256271489578079776"}},"outputId":"75c7116f-54d3-492d-cb3b-bf093a0154d9"},"source":["# To remove the tags from the above output we can use either text or string \n","soup.title.string"],"execution_count":38,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'List of Asian countries by area - Wikipedia'"]},"metadata":{"tags":[]},"execution_count":38}]},{"cell_type":"code","metadata":{"id":"M0LX3MObVSRx"},"source":["soup.find_all(\"a\")\n","\n","# this command shows all the links under a tags."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qQy7hDTxWqyj"},"source":["# We want to fetch only the links under a tags . so we have to use iteration property to fetch all the link\n","\n","all_link=soup.find_all(\"a\")\n","\n","for link in all_link:\n","  print(link.get(\"href\"))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"baYfgynuXiQ7"},"source":["# Our objective is to extract the Country from the right table in the wiki_link.\n","# To complete these task first we have to fetch all table.\n","\n","all_table=soup.find_all(\"table\")\n","print(all_table)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"k93lhmTPcOs7"},"source":["# Now we have to identify which one is the right table. so we will use the attribute- the class of table and use it to filter the right table \n","# To know the class of the right table open the url and go to any column name or header name of the right table and then right click on it \n","# a dialog box will pop up . Click on the inspect \n","# Few seconds later on the right side a box will appear containig the html documents . Go to table which you want to fetch an look at the name of the table .\n","\n","right_table=soup.find(\"table\",class_=\"wikitable sortable\")\n","print(right_table)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rr3gcjhvhL0B"},"source":["# Since the country column which we want to fetch has one thing in common that is : all of them are enclosed within  \"a tags\" as they contain links. but also\n","# the other columns like Notes contan links. so it is going to fetch all the a tags having links in it .\n","table_links=right_table.find_all(\"a\")\n","table_links"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BcKENu5ZkVrU"},"source":["# Task is to fetch the title only from the table_links\n","\n","country=[]\n","for link in table_links:\n","  country.append(link.get(\"title\"))\n","\n","print(country)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BPfF0TMUienr","colab":{"base_uri":"https://localhost:8080/","height":419},"executionInfo":{"status":"ok","timestamp":1624964276631,"user_tz":-330,"elapsed":28,"user":{"displayName":"Shadab Alam","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgyC0IjSD4d7VtA_QxL79JaEuLCgySEzKHr-_4KUQ=s64","userId":"12256271489578079776"}},"outputId":"2df56646-26cd-4b74-e2dc-fb41e9ce1a63"},"source":["import pandas as pd\n","df=pd.DataFrame()\n","df[\"Country\"]=country\n","df\n"],"execution_count":45,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Country</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Russia</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>European Russia</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>None</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>China</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Taiwan</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>69</th>\n","      <td>Gaza Strip</td>\n","    </tr>\n","    <tr>\n","      <th>70</th>\n","      <td>Brunei</td>\n","    </tr>\n","    <tr>\n","      <th>71</th>\n","      <td>Bahrain</td>\n","    </tr>\n","    <tr>\n","      <th>72</th>\n","      <td>Singapore</td>\n","    </tr>\n","    <tr>\n","      <th>73</th>\n","      <td>Maldives</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>74 rows × 1 columns</p>\n","</div>"],"text/plain":["            Country\n","0            Russia\n","1   European Russia\n","2              None\n","3             China\n","4            Taiwan\n","..              ...\n","69       Gaza Strip\n","70           Brunei\n","71          Bahrain\n","72        Singapore\n","73         Maldives\n","\n","[74 rows x 1 columns]"]},"metadata":{"tags":[]},"execution_count":45}]},{"cell_type":"code","metadata":{"id":"ff3mIEgUZuq5","executionInfo":{"status":"ok","timestamp":1624964276637,"user_tz":-330,"elapsed":26,"user":{"displayName":"Shadab Alam","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgyC0IjSD4d7VtA_QxL79JaEuLCgySEzKHr-_4KUQ=s64","userId":"12256271489578079776"}}},"source":[""],"execution_count":45,"outputs":[]}]}